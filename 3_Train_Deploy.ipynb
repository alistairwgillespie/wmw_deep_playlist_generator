{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wilson's Morning Wake Up Playlist Generator, Modeling and Learning\n",
    "\n",
    "## CPU Only\n",
    "\n",
    "The following steps will be executed:\n",
    "\n",
    "* Upload your data to S3.\n",
    "* Define a benchmark and candidate models and training scripts\n",
    "* Train models and deploy.\n",
    "* Evaluate deployed estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # session and role\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# role = sagemaker.get_execution_role()\n",
    "\n",
    "# # create an S3 bucket\n",
    "# bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload your training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set prefix, a descriptive name for a directory  \n",
    "# prefix = 'sagemaker/wmw_estimator'\n",
    "\n",
    "# # upload all data to S3\n",
    "# input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "It's time to define and train the models!\n",
    "\n",
    "---\n",
    "\n",
    "## Complete a training script \n",
    "\n",
    "To implement a custom estimator, I need to complete a `train.py` script. \n",
    "\n",
    "A typical training script:\n",
    "* Loads training data from a specified directory\n",
    "* Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "* Instantiates a model of your design, with any specified hyperparams\n",
    "* Trains that model \n",
    "* Finally, saves the model so that it can be hosted/deployed, later\n",
    "\n",
    "### Defining and training a model\n",
    "\n",
    "To complete a `train.py` file, you will:\n",
    "1. Import any extra libraries you need\n",
    "2. Define any additional model training hyperparameters using `parser.add_argument`\n",
    "2. Define a model in the `if __name__ == '__main__':` section\n",
    "3. Train the model in that same section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Directory of train.py\n",
    "!pygmentize model/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create an Estimator\n",
    "\n",
    "When a custom model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained; the `train.py` function you specified above. To run a custom training script in SageMaker, construct an estimator, and fill in the appropriate constructor arguments:\n",
    "\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory `source_sklearn` OR `source_pytorch`.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory `train_sklearn` OR `train_pytorch`.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training.\n",
    "* **source_dir**: The path to the training script directory `train_sklearn` OR `train_pytorch`.\n",
    "* **role**: Role ARN, which was specified, above.\n",
    "* **train_instance_count**: The number of training instances (should be left at 1).\n",
    "* **train_instance_type**: The type of SageMaker instance for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* **sagemaker_session**: The session used to train on Sagemaker.\n",
    "* **hyperparameters** (optional): A dictionary `{'name':value, ..}` passed to the train function as hyperparameters.\n",
    "\n",
    "Note: For a PyTorch model, there is another optional argument **framework_version**, which you can set to the latest version of PyTorch, `1.0`.\n",
    "\n",
    "## Define PyTorch estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences and targets\n",
    "def create_playlist_sequences(input_data):\n",
    "    input_playlists = []\n",
    "    \n",
    "    for i in input_data['volume'].unique():\n",
    "        temp_vol = input_data[input_data['volume'] == i]\n",
    "        X = temp_vol.iloc[:, 2:11].values\n",
    "        y = temp_vol.iloc[:, 11:].values\n",
    "        input_playlists.append((X, y))\n",
    "        \n",
    "    return input_playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "def _print_success_message():\n",
    "    print('Tests Passed!')\n",
    "\n",
    "def test_playlist_sequences(input_playlists):\n",
    "    \n",
    "    track_features = [-2.39099487, -2.63509459, -0.27732204,  0.92969533, -0.48983686,-1.15691947,  1.08569029, -1.20454903,  2.09618458, -5.37044178, 0.23380331]\n",
    "    \n",
    "    track_features_len = 11\n",
    "    target_features_len = 8\n",
    "    \n",
    "    # check shape and equality of first track\n",
    "    assert len(input_playlists[0][0][0]) == len(track_features), \\\n",
    "        'Number of features in input_playlist features does not match expected number of ' + str(len(track_features))    \n",
    "    \n",
    "    # check shape of input and output arrays\n",
    "    assert input_playlists[0][0].shape[1]==track_features_len, \\\n",
    "        'input_features should have as many columns as selected features, got: {}'.format(train_x.shape[1])\n",
    "    assert input_playlists[0][1].shape[1]==target_features_len, \\\n",
    "        'target_features should have as many columns as selected features, got: {}'.format(train_x.shape[1])\n",
    "    \n",
    "    #TODO: Add more tests\n",
    "    \n",
    "    _print_success_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run of benchmark and candidate models and train components\n",
    "Here I will see if the configurations I have set work accordingly with no errors. Once it runs smoothly, I will instantiate an estimator using the Sagemaker API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "\n",
    "# Gather sequences and targets\n",
    "processed_data = create_playlist_sequences(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class PlaylistDataset(Dataset):\n",
    "    def __init__(self, data_dir, csv_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            img_path (string): path to the folder where images are\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        # Read the csv file\n",
    "        self.data = pd.read_csv(os.path.join(data_dir, csv_path))\n",
    "        # First column contains the image paths\n",
    "        self.data_arr = self.data.iloc[:, 2:11].values\n",
    "        # Second column is the labels\n",
    "        self.label_arr = self.data.iloc[:, 11:].values\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.data.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get track\n",
    "        single_track = torch.from_numpy(self.data_arr[index]).float()\n",
    "        \n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        single_target = torch.from_numpy(self.label_arr[index]).float()\n",
    "\n",
    "        return (single_track, single_target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call dataset\n",
    "    dataset = PlaylistDataset('data', \"tensor_train_df.csv\")\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=12, shuffle=False)\n",
    "    print(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  PlaylistDataset('data', \"tensor_train_df.csv\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for LSTM\n",
    "def train_lstm(model, train_loader, epochs, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    This is the training method that is called by the PyTorch training script of the LSTM model. The parameters\n",
    "    passed are as follows:\n",
    "    model        - The PyTorch model that we wish to train.\n",
    "    train_loader - The PyTorch DataLoader that should be used during training.\n",
    "    epochs       - The total number of epochs to train for.\n",
    "    criterion    - The loss function used for training. \n",
    "    optimizer    - The optimizer to use during training.\n",
    "    device       - Where the model and data should be loaded (gpu or cpu).\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train() # Make sure that the model is in training mode.\n",
    "    \n",
    "    # training loop is provided\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            cum_loss = 0\n",
    "            \n",
    "            for i, track in enumerate(batch):\n",
    "                \n",
    "                model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_dim), torch.zeros(1, 1, model.hidden_layer_dim))                \n",
    "\n",
    "                track_x = track[0]\n",
    "                track_y = track[-1]\n",
    "                \n",
    "                output = model(track_x.unsqueeze(0))\n",
    "                \n",
    "                loss = criterion(output, track_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                cum_loss += loss.data.item()\n",
    "\n",
    "            total_loss = cum_loss / len(batch[0])\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch: {}/{}.............'.format(epoch, epochs), end=' ')\n",
    "            print(\"Loss: {:.4f}\".format(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from model.LSTM_Estimator import LSTMEstimator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lstm_model = LSTMEstimator(9, 30, 1, 9)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "train_lstm(lstm_model, dataloader, 1500, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LSTM\n",
    "torch.save(lstm_model.state_dict(), 'artefacts/lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEstimator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNEstimator, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size) \n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        combined = torch.cat((inp, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for RNN\n",
    "def train_rnn(model, dataloader, epochs, criterion, optimizer, device):\n",
    "    \n",
    "    model.train() # Make sure that the model is in training mode.\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            \n",
    "            cum_loss = 0\n",
    "            \n",
    "            hidden = model.initHidden()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            for i, track in enumerate(batch):\n",
    "\n",
    "                track_x = track[0]\n",
    "                track_y = track[-1]\n",
    "                \n",
    "                output, hidden = model(track_x.unsqueeze(0), hidden)\n",
    "            \n",
    "                loss = criterion(output, track_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                cum_loss += loss.data.item()\n",
    "                \n",
    "            total_loss = cum_loss / len(batch[0])\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch: {}/{}.............'.format(epoch, epochs), end=' ')\n",
    "            print(\"Loss: {:.4f}\".format(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rnn_model = RNNEstimator(9, 30, 9)\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "train_rnn(rnn_model, dataloader, 1500, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RNN\n",
    "torch.save(model.state_dict(), 'artefacts/rnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spotify API\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "\n",
    "# Spotify for developers client auth variables\n",
    "username = os.environ['SPOTIFY_EMAIL']\n",
    "spotify_id = os.environ['SPOTIFY_ID']\n",
    "spotify_secret = os.environ['SPOTIFY_SECRET']\n",
    "\n",
    "# Set API scope\n",
    "scope='playlist-read-private, playlist-modify-private, playlist-modify-public'\n",
    "\n",
    "# Get auth token\n",
    "token = util.prompt_for_user_token(username, \n",
    "                                   scope,\n",
    "                                   client_id=spotify_id,\n",
    "                                   client_secret=spotify_secret,\n",
    "                                   redirect_uri='http://localhost/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "#Authenticate\n",
    "sp = spotipy.Spotify(\n",
    "    client_credentials_manager = SpotifyClientCredentials(\n",
    "        client_id=spotify_id,\n",
    "        client_secret=spotify_secret\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in WMW tracks to date for recommendations\n",
    "track_data = pd.read_csv(os.path.join(data_dir, \"wmw_tracks.csv\"))\n",
    "\n",
    "track_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "pca.fit(track_data[feature_list])\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO- Save StandarScaler for later us\n",
    "from pickle import dump\n",
    "\n",
    "# save the scaler\n",
    "dump(pca, open('artefacts/dim_red.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(pca_df[:,0], pca_df[:,1], pca_df[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pca.transform(track_data[feature_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "import torch.optim as optim\n",
    "\n",
    "from model.LSTM_Estimator import LSTMEstimator\n",
    "from model.RNN_Estimator import RNNEstimator\n",
    "\n",
    "\n",
    "class Playlist():\n",
    "    def __init__(self, wmw_pool, model_type=\"LSTM\"):\n",
    "        self.recommended_track_ids = pd.DataFrame() #list of track ids straight from spotify\n",
    "        self.trax = [] #all tracks as dict\n",
    "        self.df = None #this is where the data goes\n",
    "        self.playlist = None\n",
    "        self.wmw_pool_df = wmw_pool\n",
    "        \n",
    "        # Feature set\n",
    "        self.feature_list =  ['danceability','energy', 'loudness', 'speechiness', 'acousticness',\n",
    "                         'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "    \n",
    "        # Setup feature standardisation\n",
    "        self.std_scaler = joblib.load('artefacts/standard_features.pkl')\n",
    "        \n",
    "        # Setup dimensionality reduction for track picking\n",
    "        self.dim_red = joblib.load('artefacts/dim_red.pkl')\n",
    "        \n",
    "        if model_type == \"LSTM\":\n",
    "            model = LSTMEstimator(9, 30, 1, 9)\n",
    "            model.load_state_dict(torch.load('artefacts/lstm_model.pth'))\n",
    "            \n",
    "        elif model_type == \"RNN\":\n",
    "            model = RNNEstimator(9, 30, 9)\n",
    "            model.load_state_dict(torch.load('artefacts/rnn_model.pth'))\n",
    "        else:\n",
    "            print(\"Please specify either the RNN or LSTM model using the model_type parameter.\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Start building the new playlist\n",
    "        self.intro_track = self.get_first_track()\n",
    "        self.new_playlist = self.predict_playlist(model, self.intro_track)\n",
    "\n",
    "    \n",
    "    def get_first_track(self):\n",
    "        \"\"\"Get first track based on recommendations.\"\"\"\n",
    "        # Sample an intro song from the WMW history\n",
    "        song = self.wmw_pool_df[self.wmw_pool_df['position'] == 1].sample(1).copy()\n",
    "\n",
    "        # Gather a recommendation based on the intro track using spotify\n",
    "        song_res = sp.recommendations(seed_tracks = song['id'].values, limit=1)\n",
    "        \n",
    "        # Gather track freatures from spotify result\n",
    "        for r in song_res['tracks']:\n",
    "            track={}\n",
    "            track['id'] = r['id']\n",
    "            track['artists'] = [i['name'] for i in r['artists']],\n",
    "            track['name'] = r['name']\n",
    "            track_features = sp.audio_features(r['id'])[0]\n",
    "            track.update(track_features)\n",
    "            self.intro_track = pd.DataFrame(track, index=[0])\n",
    "\n",
    "        # Prepare features\n",
    "        self.intro_track[self.feature_list] = self.std_scaler.transform(self.intro_track[self.feature_list])\n",
    "        \n",
    "        return self.intro_track\n",
    "    \n",
    "    def harmonic_match(self, key, mode):\n",
    "        \"\"\"Given a key and mode, return compatible keys according to the harmonic wheel.\"\"\"\n",
    "        \n",
    "        # Harmonic Mixing Wheel: Pitch Class \n",
    "        # 1A 0 - A flat minor: 8 | 1B 0 - B major: 11\n",
    "        # 2A 1 - E flat minor: 3 | 2B 1 - F-sharp major: 6\n",
    "        # 3A 2 - B-flat minor: 10 | 3B 2 - D-flat major: 1\n",
    "        # 4A 3 - F minor: 5 | 4B 3 - A-flat major: 8\n",
    "        # 5A 4 - C minor: 0 | 5B 4 - E-flat major: 3\n",
    "        # 6A 5 - G minor: 7 | 6B 5 - B-flat major: 10\n",
    "        # 7A 6 - D minor: 2 | 7B 6 - F major: 5\n",
    "        # 8A 7 - A minor: 9 | 8B 7 - C major: 0\n",
    "        # 9A 8 - E minor: 4 | 9B 8 - G major: 7\n",
    "        # 10A 9 - B minor: 11 | 10B 9 - D major: 2\n",
    "        # 11A 10 - F sharp minor: 6 | 11B 10 - A major: 9\n",
    "        # 12A 11 - D flat minor: 1 | 12B 11 - E major: 4\n",
    "\n",
    "        # Harmonic keys mapped to corresponding pitch classes\n",
    "        pitch_to_harmonic_keys = {0: [4, 7], 1: [11, 2], 2: [6, 9],\n",
    "                                  3: [1, 4], 4: [8, 11], 5: [3, 6],\n",
    "                                  6: [10, 1], 7: [5, 8], 8: [0, 3],\n",
    "                                  9: [7, 10], 10: [2, 5], 11: [9, 0]}\n",
    "\n",
    "        # Extract values and keys\n",
    "        dv = np.array(list(pitch_to_harmonic_keys.values()))\n",
    "        dk = np.array(list(pitch_to_harmonic_keys.keys()))\n",
    "\n",
    "        # Harmonic key code corresponding song pitch class\n",
    "        harm_key = dv[np.where(dk == key)][0][mode]\n",
    "\n",
    "        # Harmonic key codes\n",
    "        harmonic_keys = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "\n",
    "        # Get compatible key codes\n",
    "        comp_keycodes = np.take(harmonic_keys, \n",
    "                                [harm_key - 1, harm_key, harm_key + 1],\n",
    "                                mode='wrap')\n",
    "\n",
    "        # Compatible keys\n",
    "        comp_keys = [np.where(dv[:, mode] == i)[0][0].tolist() for i in comp_keycodes]\n",
    "\n",
    "        # Compatible up/down key\n",
    "        inner_outer_key = np.array([np.where(dv[:, int(not bool(mode))] == harm_key)[0][0]])\n",
    "\n",
    "        comp_keys = np.concatenate([comp_keys, inner_outer_key])\n",
    "        \n",
    "        return comp_keys, inner_outer_key\n",
    "    \n",
    "    \n",
    "    def get_position_recommendations(self, track_position):\n",
    "        \"\"\"Obtain a dataframe of recommended tracks for a specific track position.\"\"\"\n",
    "        \n",
    "        recommendations = pd.DataFrame()\n",
    "\n",
    "        wmw_sample = random.sample(self.wmw_pool_df['volume'].unique().tolist(), 10)\n",
    "\n",
    "        wmw_sample_df = self.wmw_pool_df[\n",
    "            (self.wmw_pool_df['volume'].isin(wmw_sample)) & \n",
    "            (self.wmw_pool_df['position'] == track_position)\n",
    "        ]\n",
    "\n",
    "#         wmw_sample_df = wmw_sample_df[].copy()\n",
    "\n",
    "        # Iterate full catalog of WMW songs\n",
    "        for _, row in wmw_sample_df.iterrows():\n",
    "            \n",
    "            song_search = row['track_name'].partition('-')[0] + ' ' + row['artist_name']\n",
    "            \n",
    "            try:\n",
    "\n",
    "                # Query Spotify to get track metadata\n",
    "                song_res = sp.search(song_search, limit=1)['tracks']['items'][0]\n",
    "\n",
    "                # Gather recommendations for each of the past WMW tracks\n",
    "                results = sp.recommendations(seed_tracks = [song_res['id']], limit=20)\n",
    "\n",
    "                for r in results['tracks']:\n",
    "                    track={}\n",
    "                    track['id'] = r['id']\n",
    "                    track['artists'] = [i['name'] for i in r['artists']],\n",
    "                    track['name'] = r['name']\n",
    "                    track_features = sp.audio_features(r['id'])[0]\n",
    "                    track.update(track_features)\n",
    "                    final_track = pd.DataFrame(track, index=[0])\n",
    "                    recommendations = recommendations.append(final_track, ignore_index=True)\n",
    "\n",
    "            except:\n",
    "                print(\"Song not searchable\")\n",
    "\n",
    "        recommendations[self.feature_list] = self.std_scaler.transform(recommendations[self.feature_list])\n",
    "\n",
    "        return recommendations\n",
    "    \n",
    "    def pick_optimal_track(self, candidates, target):\n",
    "        \"\"\"Select the track with the minimum distance between the candidate tracks.\"\"\"\n",
    "        \n",
    "        candidates_reduced = self.dim_red.transform(candidates[self.feature_list])\n",
    "        \n",
    "        target_reduced = self.dim_red.transform(target)\n",
    "        \n",
    "        next_track_id = np.argmin(cdist(target_reduced, candidates_reduced))\n",
    "        \n",
    "        next_track = candidates.iloc[next_track_id]\n",
    "        \n",
    "        return next_track\n",
    "    \n",
    "\n",
    "    def predict_playlist(self, model, intro_track, playlist_len=15):\n",
    "        \"\"\"Predict playlist\"\"\"\n",
    "        \n",
    "        # Prepare prediction list\n",
    "        predicted = intro_track\n",
    "        \n",
    "        # Prepare initial input \n",
    "        inp = torch.FloatTensor(intro_track[self.feature_list].values)\n",
    "        \n",
    "        print(\"Intro track:\", predicted['name'].values[0], '-', ', '.join(predicted['artists'].values[0]))\n",
    "\n",
    "        for p in tqdm(range(2, playlist_len + 1)):\n",
    "            print(\"Track #%s - Generating candidates\" % p)\n",
    "            \n",
    "            # Important stuff about the last track\n",
    "            current_track = predicted.iloc[-1]\n",
    "            current_key = current_track['key']\n",
    "            current_mode = current_track['mode']\n",
    "\n",
    "            # Generate output feature set of next song\n",
    "            output = model(inp).detach().numpy()\n",
    "\n",
    "            # Get mode and key from last song and generate compatible keys and modes\n",
    "            keys, outer_inner_key = self.harmonic_match(current_key, current_mode)\n",
    "\n",
    "            # Get recommended tracks for current track position\n",
    "            recommendations = self.get_position_recommendations(p)\n",
    "            \n",
    "            print(\"Recommendations\", recommendations.shape)\n",
    "            \n",
    "            # Filter for compatible tracks according to key and mode (harmonic wheel)\n",
    "            next_tracks_curr_mode = recommendations[\n",
    "                (recommendations['key'].isin(keys[:3])) & (recommendations['mode'] == current_mode)\n",
    "            ]\n",
    "            \n",
    "            print(\"Curr mode\", next_tracks_curr_mode.shape)\n",
    "            \n",
    "            next_tracks_change_mode = recommendations[\n",
    "                (recommendations['key'] == keys[-1]) & (recommendations['mode'] == abs(int(not current_mode)))\n",
    "            ]\n",
    "            \n",
    "            print(\"Change mode\", next_tracks_change_mode.shape)\n",
    "            \n",
    "            candidate_tracks = pd.concat([next_tracks_curr_mode, next_tracks_change_mode]).reset_index(drop=True)\n",
    "            \n",
    "            # Ensure no duplicates exist in the playlist\n",
    "            candidate_tracks = candidate_tracks[~candidate_tracks['id'].isin(predicted['id'])]\n",
    "            \n",
    "            print(\"CANDIDATES:\", candidate_tracks.shape)\n",
    "            \n",
    "            # Pick optimal track\n",
    "            next_track = self.pick_optimal_track(candidate_tracks, output)\n",
    "            \n",
    "            print(\"Selected:\", next_track['name'], '-', ', '.join(next_track['artists']))\n",
    "\n",
    "            # Set new input vector for next song\n",
    "            inp = torch.FloatTensor([next_track[self.feature_list]])\n",
    "\n",
    "            # Append next song to playlist\n",
    "            predicted = predicted.append(next_track, ignore_index=True)\n",
    "            \n",
    "            print('-' * 20)\n",
    "\n",
    "        return predicted\n",
    "    \n",
    "\n",
    "    def post_playlist(self):\n",
    "        if token:\n",
    "            sp = spotipy.Spotify(auth=token)\n",
    "            sp.trace = False\n",
    "            tracks = sp.user_playlist_replace_tracks('1247785541', '7x1MY3AW3YCaHoicpiacGv', self.new_playlist['id'].values)\n",
    "            print(\"Posting latest Wilson's FM.\")\n",
    "        else:\n",
    "            print(\"Can't get token for\", username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl = Playlist(track_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if token:\n",
    "    sp = spotipy.Spotify(auth=token)\n",
    "    sp.trace = False\n",
    "    tracks = sp.user_playlist_replace_tracks('1247785541', '7x1MY3AW3YCaHoicpiacGv', pl.new_playlist['id'].values)\n",
    "else:\n",
    "    print(\"Can't get token for\", username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "def _print_success_message():\n",
    "    print('Tests Passed!')\n",
    "\n",
    "def test_harmonic_mixing(song):\n",
    "    \n",
    "    truth_octaves = [11, 0, 1]\n",
    "    \n",
    "    next_octaves = harmonic_match(0, 1)\n",
    "    \n",
    "    # check shape and equality of first track\n",
    "    assert len(truth_octaves) == len(next_octaves), \\\n",
    "        'Number of octaves incorrect, should get: ' + str(len(truth_octaves))    \n",
    "    \n",
    "    # check shape of input and output arrays\n",
    "    assert input_playlists[0][0].shape[1]==track_features_len, \\\n",
    "        'input_features should have as many columns as selected features, got: {}'.format(train_x.shape[1])\n",
    "    assert input_playlists[0][1].shape[1]==target_features_len, \\\n",
    "        'target_features should have as many columns as selected features, got: {}'.format(train_x.shape[1])\n",
    "    \n",
    "    #TODO: Add more tests\n",
    "    \n",
    "    _print_success_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the PyTorch Model with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator code\n",
    "from sagemaker.pytorch import PyTorch\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "estimator = PyTorch(entry_point=\"LSTM_Train.py\",\n",
    "                    source_dir=\"model\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    output_path = output_path,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'input_features': 11,\n",
    "                        'hidden_dim': 12,\n",
    "                        'output_dim': 8,\n",
    "                        'epochs': 100\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit estimator\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_wmw",
   "language": "python",
   "name": "local_wmw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
