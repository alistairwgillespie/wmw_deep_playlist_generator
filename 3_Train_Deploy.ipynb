{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wilson's Morning Wake Up Playlist Generator, Modeling and Learning\n",
    "\n",
    "The following steps will be executed:\n",
    "\n",
    "* Upload your data to S3.\n",
    "* Define a benchmark and candidate models and training scripts\n",
    "* Train models and deploy.\n",
    "* Evaluate deployed estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload your training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'data'\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'sagemaker/wmw_estimator'\n",
    "\n",
    "# upload all data to S3\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "It's time to define and train the models!\n",
    "\n",
    "---\n",
    "\n",
    "## Complete a training script \n",
    "\n",
    "To implement a custom estimator, I need to complete a `train.py` script. \n",
    "\n",
    "A typical training script:\n",
    "* Loads training data from a specified directory\n",
    "* Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "* Instantiates a model of your design, with any specified hyperparams\n",
    "* Trains that model \n",
    "* Finally, saves the model so that it can be hosted/deployed, later\n",
    "\n",
    "### Defining and training a model\n",
    "\n",
    "To complete a `train.py` file, you will:\n",
    "1. Import any extra libraries you need\n",
    "2. Define any additional model training hyperparameters using `parser.add_argument`\n",
    "2. Define a model in the `if __name__ == '__main__':` section\n",
    "3. Train the model in that same section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import subprocess as sb \n",
      "import sys \n",
      "\n",
      "sb.call([sys.executable, \"-m\", \"pip\", \"install\", 'pandas']) \n",
      "\n",
      "import argparse\n",
      "import json\n",
      "import os\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torch.utils.data\n",
      "\n",
      "# imports the model in model.py by name\n",
      "from model import BinaryClassifier\n",
      "\n",
      "\n",
      "def model_fn(model_dir):\n",
      "    \"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\n",
      "    print(\"Loading model.\")\n",
      "\n",
      "    # First, load the parameters used to create the model.\n",
      "    model_info = {}\n",
      "    model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
      "    with open(model_info_path, 'rb') as f:\n",
      "        model_info = torch.load(f)\n",
      "\n",
      "    print(\"model_info: {}\".format(model_info))\n",
      "\n",
      "    # Determine the device and construct the model.\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "    model = BinaryClassifier(model_info['input_features'], model_info['hidden_dim'], model_info['output_dim'])\n",
      "\n",
      "    # Load the stored model parameters.\n",
      "    model_path = os.path.join(model_dir, 'model.pth')\n",
      "    with open(model_path, 'rb') as f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "\n",
      "    # set to eval mode, could use no_grad\n",
      "    model.to(device).eval()\n",
      "\n",
      "    print(\"Done loading model.\")\n",
      "    return model\n",
      "\n",
      "# Gets training data in batches from the train.csv file\n",
      "def _get_train_data_loader(batch_size, training_dir):\n",
      "    print(\"Get train data loader.\")\n",
      "\n",
      "    train_data = pd.read_csv(os.path.join(training_dir, \"train.csv\"), header=None, names=None)\n",
      "\n",
      "    train_y = torch.from_numpy(train_data[[0]].values).float().squeeze()\n",
      "    train_x = torch.from_numpy(train_data.drop([0], axis=1).values).float()\n",
      "\n",
      "    train_ds = torch.utils.data.TensorDataset(train_x, train_y)\n",
      "\n",
      "    return torch.utils.data.DataLoader(train_ds, batch_size=batch_size)\n",
      "\n",
      "\n",
      "# Provided training function\n",
      "def train(model, train_loader, epochs, criterion, optimizer, device):\n",
      "    \"\"\"\n",
      "    This is the training method that is called by the PyTorch training script. The parameters\n",
      "    passed are as follows:\n",
      "    model        - The PyTorch model that we wish to train.\n",
      "    train_loader - The PyTorch DataLoader that should be used during training.\n",
      "    epochs       - The total number of epochs to train for.\n",
      "    criterion    - The loss function used for training. \n",
      "    optimizer    - The optimizer to use during training.\n",
      "    device       - Where the model and data should be loaded (gpu or cpu).\n",
      "    \"\"\"\n",
      "    \n",
      "    # training loop is provided\n",
      "    for epoch in range(1, epochs + 1):\n",
      "        model.train() # Make sure that the model is in training mode.\n",
      "\n",
      "        total_loss = 0\n",
      "\n",
      "        for batch in train_loader:\n",
      "            # get data\n",
      "            batch_x, batch_y = batch\n",
      "\n",
      "            batch_x = batch_x.to(device)\n",
      "            batch_y = batch_y.to(device)\n",
      "\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            # get predictions from model\n",
      "            y_pred = model(batch_x)\n",
      "            \n",
      "            # perform backprop\n",
      "            loss = criterion(y_pred, batch_y)\n",
      "            loss.backward()\n",
      "            optimizer.step()\n",
      "            \n",
      "            total_loss += loss.data.item()\n",
      "\n",
      "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))\n",
      "\n",
      "\n",
      "## TODO: Complete the main code\n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    # All of the model parameters and training parameters are sent as arguments\n",
      "    # when this script is executed, during a training job\n",
      "    \n",
      "    # Here we set up an argument parser to easily access the parameters\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    # SageMaker parameters, like the directories for training data and saving models; set automatically\n",
      "    # Do not need to change\n",
      "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
      "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
      "    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
      "    \n",
      "    # Training Parameters, given\n",
      "    parser.add_argument('--batch-size', type=int, default=10, metavar='N',\n",
      "                        help='input batch size for training (default: 10)')\n",
      "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
      "                        help='number of epochs to train (default: 10)')\n",
      "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
      "                        help='random seed (default: 1)')\n",
      "    \n",
      "    ## TODO: Add args for the three model parameters: input_features, hidden_dim, output_dim\n",
      "    # Model Parameters\n",
      "    \n",
      "    parser.add_argument('--input_features', type=int, default=3, metavar='N',\n",
      "                        help='size of the feature space (default: 2)')\n",
      "    parser.add_argument('--hidden_dim', type=int, default=2, metavar='N',\n",
      "                        help='size of the hidden dimension (default: 100)')\n",
      "    parser.add_argument('--output_dim', type=int, default=1, metavar='N',\n",
      "                        help='size of the output dimension (default: 4)')\n",
      "    parser.add_argument('--lr', type=float, default=1e-3,\n",
      "                        help='learning rate (default: 1e-3)')\n",
      "    \n",
      "    # args holds all passed-in arguments\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "    print(\"Using device {}.\".format(device))\n",
      "\n",
      "    torch.manual_seed(args.seed)\n",
      "\n",
      "    # Load the training data.\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir)\n",
      "\n",
      "\n",
      "    ## --- Your code here --- ##\n",
      "    \n",
      "    ## TODO:  Build the model by passing in the input params\n",
      "    # To get params from the parser, call args.argument_name, ex. args.epochs or ards.hidden_dim\n",
      "    # Don't forget to move your model .to(device) to move to GPU , if appropriate\n",
      "    model = BinaryClassifier(args.input_features, args.hidden_dim, args.output_dim).to(device)\n",
      "\n",
      "    ## TODO: Define an optimizer and loss function for training\n",
      "    optimizer = optim.Adam(model.parameters(), args.lr)\n",
      "    criterion = torch.nn.BCELoss()\n",
      "\n",
      "    # Trains the model (given line of code, which calls the above training function)\n",
      "    train(model, train_loader, args.epochs, criterion, optimizer, device)\n",
      "\n",
      "    ## TODO: complete in the model_info by adding three argument names, the first is given\n",
      "    # Keep the keys of this dictionary as they are \n",
      "    model_info_path = os.path.join(args.model_dir, 'model_info.pth')\n",
      "    with open(model_info_path, 'wb') as f:\n",
      "        model_info = {\n",
      "            'input_features': args.input_features,\n",
      "            'hidden_dim': args.hidden_dim,\n",
      "            'output_dim': args.output_dim\n",
      "        }\n",
      "        torch.save(model_info, f)\n",
      "        \n",
      "    ## --- End of your code  --- ##\n",
      "    \n",
      "\n",
      "\t# Save the model parameters\n",
      "    model_path = os.path.join(args.model_dir, 'model.pth')\n",
      "    with open(model_path, 'wb') as f:\n",
      "        torch.save(model.cpu().state_dict(), f)\n"
     ]
    }
   ],
   "source": [
    "# Directory of train.py\n",
    "!pygmentize model/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create an Estimator\n",
    "\n",
    "When a custom model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained; the `train.py` function you specified above. To run a custom training script in SageMaker, construct an estimator, and fill in the appropriate constructor arguments:\n",
    "\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory `source_sklearn` OR `source_pytorch`.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory `train_sklearn` OR `train_pytorch`.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training.\n",
    "* **source_dir**: The path to the training script directory `train_sklearn` OR `train_pytorch`.\n",
    "* **role**: Role ARN, which was specified, above.\n",
    "* **train_instance_count**: The number of training instances (should be left at 1).\n",
    "* **train_instance_type**: The type of SageMaker instance for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* **sagemaker_session**: The session used to train on Sagemaker.\n",
    "* **hyperparameters** (optional): A dictionary `{'name':value, ..}` passed to the train function as hyperparameters.\n",
    "\n",
    "Note: For a PyTorch model, there is another optional argument **framework_version**, which you can set to the latest version of PyTorch, `1.0`.\n",
    "\n",
    "## Define PyTorch estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(data_dir, \"train.csv\"), header=None, names=None)\n",
    "\n",
    "train_y = torch.from_numpy(train_data[[0]].values).float().squeeze()\n",
    "train_x = torch.from_numpy(train_data.drop([0], axis=1).values).float()\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_ds, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided training function\n",
    "def train(model, train_loader, epochs, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    This is the training method that is called by the PyTorch training script. The parameters\n",
    "    passed are as follows:\n",
    "    model        - The PyTorch model that we wish to train.\n",
    "    train_loader - The PyTorch DataLoader that should be used during training.\n",
    "    epochs       - The total number of epochs to train for.\n",
    "    criterion    - The loss function used for training. \n",
    "    optimizer    - The optimizer to use during training.\n",
    "    device       - Where the model and data should be loaded (gpu or cpu).\n",
    "    \"\"\"\n",
    "    \n",
    "    # training loop is provided\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() # Make sure that the model is in training mode.\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # get data\n",
    "            batch_x, batch_y = batch\n",
    "\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get predictions from model\n",
    "            y_pred = model(batch_x)\n",
    "            \n",
    "            # perform backprop\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from source_pytorch.model import BinaryClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryClassifier(3, 30, 1).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 100, loss_fn, optimizer, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
