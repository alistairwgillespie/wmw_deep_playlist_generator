{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wilson's Morning Wake Up Playlist Generator, Modeling and Learning\n",
    "\n",
    "The following steps will be executed:\n",
    "\n",
    "* Upload your data to S3.\n",
    "* Define a benchmark and candidate models and training scripts\n",
    "* Train models and deploy.\n",
    "* Evaluate deployed estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 312\r\n",
      "drwxrwxr-x 2 ec2-user ec2-user   4096 Mar  4 23:01 .\r\n",
      "drwxrwxr-x 8 ec2-user ec2-user   4096 Mar 18 09:06 ..\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  28467 Mar  4 23:01 test.csv\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 113732 Mar 17 10:29 train.csv\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 166951 Mar 17 04:39 wmw_tracks.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload your training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'data'\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'sagemaker/wmw_estimator'\n",
    "\n",
    "# upload all data to S3\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "It's time to define and train the models!\n",
    "\n",
    "---\n",
    "\n",
    "## Complete a training script \n",
    "\n",
    "To implement a custom estimator, I need to complete a `train.py` script. \n",
    "\n",
    "A typical training script:\n",
    "* Loads training data from a specified directory\n",
    "* Parses any training & model hyperparameters (ex. nodes in a neural network, training epochs, etc.)\n",
    "* Instantiates a model of your design, with any specified hyperparams\n",
    "* Trains that model \n",
    "* Finally, saves the model so that it can be hosted/deployed, later\n",
    "\n",
    "### Defining and training a model\n",
    "\n",
    "To complete a `train.py` file, you will:\n",
    "1. Import any extra libraries you need\n",
    "2. Define any additional model training hyperparameters using `parser.add_argument`\n",
    "2. Define a model in the `if __name__ == '__main__':` section\n",
    "3. Train the model in that same section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: cannot read infile: [Errno 2] No such file or directory: 'model/train.py'\r\n"
     ]
    }
   ],
   "source": [
    "# Directory of train.py\n",
    "!pygmentize model/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create an Estimator\n",
    "\n",
    "When a custom model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained; the `train.py` function you specified above. To run a custom training script in SageMaker, construct an estimator, and fill in the appropriate constructor arguments:\n",
    "\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory `source_sklearn` OR `source_pytorch`.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training and prediction.\n",
    "* **source_dir**: The path to the training script directory `train_sklearn` OR `train_pytorch`.\n",
    "* **entry_point**: The path to the Python script SageMaker runs for training.\n",
    "* **source_dir**: The path to the training script directory `train_sklearn` OR `train_pytorch`.\n",
    "* **role**: Role ARN, which was specified, above.\n",
    "* **train_instance_count**: The number of training instances (should be left at 1).\n",
    "* **train_instance_type**: The type of SageMaker instance for training. Note: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* **sagemaker_session**: The session used to train on Sagemaker.\n",
    "* **hyperparameters** (optional): A dictionary `{'name':value, ..}` passed to the train function as hyperparameters.\n",
    "\n",
    "Note: For a PyTorch model, there is another optional argument **framework_version**, which you can set to the latest version of PyTorch, `1.0`.\n",
    "\n",
    "## Define PyTorch estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences and targets\n",
    "def create_playlist_sequences(input_data):\n",
    "    input_playlists = []\n",
    "    \n",
    "    for i in input_data['volume'].unique():\n",
    "        temp_vol = input_data[input_data['volume'] == i]\n",
    "        playlist_X = temp_vol.iloc[:, 2:].values\n",
    "        labels_y = temp_vol.iloc[:, 2:-3].values\n",
    "        input_playlists.append((playlist_X, labels_y))\n",
    "        \n",
    "    return input_playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "def _print_success_message():\n",
    "    print('Tests Passed!')\n",
    "\n",
    "def test_playlist_sequences(input_playlists):\n",
    "    \n",
    "    track_features = [-2.39099487, -2.63509459, -0.27732204,  0.92969533, -0.48983686,-1.15691947,  1.08569029, -1.20454903,  2.09618458, -5.37044178, 0.23380331]\n",
    "    \n",
    "    track_features_len = 11\n",
    "    target_features_len = 8\n",
    "    \n",
    "    # check shape and equality of first track\n",
    "    assert len(input_playlists[0][0][0]) == len(track_features), \\\n",
    "        'Number of features in input_playlist features does not match expected number of ' + str(len(track_features))    \n",
    "    \n",
    "    # check shape of input and output arrays\n",
    "    assert input_playlists[0][0].shape[1]==track_features_len, \\\n",
    "        'input_features should have as many columns as selected features, got: {}'.format(train_x.shape[1])\n",
    "    assert input_playlists[0][1].shape[1]==target_features_len, \\\n",
    "        'target_features should have as many columns as selected features, got: {}'.format(train_x.shape[1])\n",
    "    \n",
    "    #TODO: Add more tests\n",
    "    \n",
    "    _print_success_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run of benchmark and candidate models and train components\n",
    "Here I will see if the configurations I have set work accordingly with no errors. Once it runs smoothly, I will instantiate an estimator using the Sagemaker API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "\n",
    "# Gather sequences and targets\n",
    "processed_data = create_playlist_sequences(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for LSTM\n",
    "def train_lstm(model, train_loader, epochs, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    This is the training method that is called by the PyTorch training script of the LSTM model. The parameters\n",
    "    passed are as follows:\n",
    "    model        - The PyTorch model that we wish to train.\n",
    "    train_loader - The PyTorch DataLoader that should be used during training.\n",
    "    epochs       - The total number of epochs to train for.\n",
    "    criterion    - The loss function used for training. \n",
    "    optimizer    - The optimizer to use during training.\n",
    "    device       - Where the model and data should be loaded (gpu or cpu).\n",
    "    \"\"\"\n",
    "    \n",
    "    # training loop is provided\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() # Make sure that the model is in training mode.\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            # get data\n",
    "            batch_x, batch_y = batch\n",
    "            \n",
    "            # \n",
    "            batch_x = torch.from_numpy(batch_x).float().squeeze()\n",
    "            batch_y = torch.from_numpy(batch_y).float()\n",
    "\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_dim),\n",
    "                torch.zeros(1, 1, model.hidden_layer_dim))\n",
    "\n",
    "            # get predictions from model\n",
    "            y_pred = model(batch_x)\n",
    "            \n",
    "            # perform backprop\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "            \n",
    "        if epoch%25 == 1:\n",
    "            print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.7833778133263459\n",
      "Epoch: 26, Loss: 0.08652352038267497\n",
      "Epoch: 51, Loss: 0.05532855812359501\n",
      "Epoch: 76, Loss: 0.04158522963926599\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from model.LSTM_Estimator import LSTMEstimator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMEstimator(11, 30, 1, 8)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "train_lstm(model, processed_data, 100, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'hidden2target',\n",
       " 'hidden_cell',\n",
       " 'hidden_layer_dim',\n",
       " 'hidden_layers',\n",
       " 'load_state_dict',\n",
       " 'lstm',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPOTIFY_EMAIL=gillaw06@gmail.com\n"
     ]
    }
   ],
   "source": [
    "%env SPOTIFY_EMAIL=gillaw06@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPOTIFY_ID=ce1d1ca394724265951a48a0deea6d01\n"
     ]
    }
   ],
   "source": [
    "%env SPOTIFY_ID=ce1d1ca394724265951a48a0deea6d01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPOTIFY_SECRET=3ce5bb4c8c18423f9e8b3f12db963e31\n"
     ]
    }
   ],
   "source": [
    "%env SPOTIFY_SECRET=3ce5bb4c8c18423f9e8b3f12db963e31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spotipy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from spotipy) (1.11.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from spotipy) (2.20.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.20.0->spotipy) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.20.0->spotipy) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.20.0->spotipy) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.20.0->spotipy) (3.0.4)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spotify API\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "\n",
    "# Defaults\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Spotify for developers client auth variables\n",
    "username = os.environ['SPOTIFY_EMAIL']\n",
    "spotify_id = os.environ['SPOTIFY_ID']\n",
    "spotify_secret = os.environ['SPOTIFY_SECRET']\n",
    "\n",
    "# Set API scope\n",
    "scope='playlist-read-private'\n",
    "\n",
    "# Get auth token\n",
    "token = util.prompt_for_user_token(username, \n",
    "                                   scope,\n",
    "                                   client_id=spotify_id,\n",
    "                                   client_secret=spotify_secret,\n",
    "                                   redirect_uri='http://localhost/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Authenticate\n",
    "sp = spotipy.Spotify(\n",
    "    client_credentials_manager = SpotifyClientCredentials(\n",
    "        client_id=spotify_id,\n",
    "        client_secret=spotify_secret\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>position</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>Finding It There</td>\n",
       "      <td>Goldmund</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.00257</td>\n",
       "      <td>1</td>\n",
       "      <td>-37.134</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>123.707</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6CnPCuUcM3A5PMP4gUy0vw</td>\n",
       "      <td>spotify:track:6CnPCuUcM3A5PMP4gUy0vw</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6CnPCuUcM3A5...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6CnP...</td>\n",
       "      <td>220120</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>Light Forms</td>\n",
       "      <td>Rohne</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.54500</td>\n",
       "      <td>10</td>\n",
       "      <td>-12.848</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>133.036</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6MkUPsz5hYeneo0a9H0VT8</td>\n",
       "      <td>spotify:track:6MkUPsz5hYeneo0a9H0VT8</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6MkUPsz5hYen...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6MkU...</td>\n",
       "      <td>265870</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>C-Side</td>\n",
       "      <td>Khruangbin</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.77900</td>\n",
       "      <td>11</td>\n",
       "      <td>-10.129</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.9380</td>\n",
       "      <td>94.073</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6GvAM8oyVApQHGMgpBt8yl</td>\n",
       "      <td>spotify:track:6GvAM8oyVApQHGMgpBt8yl</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6GvAM8oyVApQ...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6GvA...</td>\n",
       "      <td>283407</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>Didn't I (Dave Allison Rework)</td>\n",
       "      <td>Darondo</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.70500</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.729</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.6850</td>\n",
       "      <td>186.033</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>1owjOeZt1BdYWW6T8fIAEe</td>\n",
       "      <td>spotify:track:1owjOeZt1BdYWW6T8fIAEe</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1owjOeZt1BdY...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1owj...</td>\n",
       "      <td>328000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>Woman Of The Ghetto - Akshin Alizadeh Remix</td>\n",
       "      <td>Marlena Shaw</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.57300</td>\n",
       "      <td>7</td>\n",
       "      <td>-8.403</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>100.006</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>2h8cQH7zhUWrynZi2MKhhC</td>\n",
       "      <td>spotify:track:2h8cQH7zhUWrynZi2MKhhC</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2h8cQH7zhUWr...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2h8c...</td>\n",
       "      <td>302467</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   volume  position                                   track_name  \\\n",
       "0      38         1                             Finding It There   \n",
       "1      38         2                                  Light Forms   \n",
       "2      38         3                                       C-Side   \n",
       "3      38         4               Didn't I (Dave Allison Rework)   \n",
       "4      38         5  Woman Of The Ghetto - Akshin Alizadeh Remix   \n",
       "\n",
       "    artist_name  danceability   energy  key  loudness  mode  speechiness  ...  \\\n",
       "0      Goldmund         0.187  0.00257    1   -37.134     1       0.0427  ...   \n",
       "1         Rohne         0.671  0.54500   10   -12.848     0       0.0393  ...   \n",
       "2    Khruangbin         0.688  0.77900   11   -10.129     0       0.0579  ...   \n",
       "3       Darondo         0.539  0.70500    0    -6.729     1       0.0527  ...   \n",
       "4  Marlena Shaw         0.707  0.57300    7    -8.403     0       0.0276  ...   \n",
       "\n",
       "   liveness  valence    tempo            type                      id  \\\n",
       "0    0.0915   0.0374  123.707  audio_features  6CnPCuUcM3A5PMP4gUy0vw   \n",
       "1    0.1180   0.2840  133.036  audio_features  6MkUPsz5hYeneo0a9H0VT8   \n",
       "2    0.3490   0.9380   94.073  audio_features  6GvAM8oyVApQHGMgpBt8yl   \n",
       "3    0.1330   0.6850  186.033  audio_features  1owjOeZt1BdYWW6T8fIAEe   \n",
       "4    0.0858   0.1890  100.006  audio_features  2h8cQH7zhUWrynZi2MKhhC   \n",
       "\n",
       "                                    uri  \\\n",
       "0  spotify:track:6CnPCuUcM3A5PMP4gUy0vw   \n",
       "1  spotify:track:6MkUPsz5hYeneo0a9H0VT8   \n",
       "2  spotify:track:6GvAM8oyVApQHGMgpBt8yl   \n",
       "3  spotify:track:1owjOeZt1BdYWW6T8fIAEe   \n",
       "4  spotify:track:2h8cQH7zhUWrynZi2MKhhC   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/6CnPCuUcM3A5...   \n",
       "1  https://api.spotify.com/v1/tracks/6MkUPsz5hYen...   \n",
       "2  https://api.spotify.com/v1/tracks/6GvAM8oyVApQ...   \n",
       "3  https://api.spotify.com/v1/tracks/1owjOeZt1BdY...   \n",
       "4  https://api.spotify.com/v1/tracks/2h8cQH7zhUWr...   \n",
       "\n",
       "                                        analysis_url duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/6CnP...      220120   \n",
       "1  https://api.spotify.com/v1/audio-analysis/6MkU...      265870   \n",
       "2  https://api.spotify.com/v1/audio-analysis/6GvA...      283407   \n",
       "3  https://api.spotify.com/v1/audio-analysis/1owj...      328000   \n",
       "4  https://api.spotify.com/v1/audio-analysis/2h8c...      302467   \n",
       "\n",
       "  time_signature  \n",
       "0              5  \n",
       "1              4  \n",
       "2              4  \n",
       "3              4  \n",
       "4              4  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in WMW tracks to date for recommendations\n",
    "track_data = pd.read_csv(os.path.join(data_dir, \"wmw_tracks.csv\"))\n",
    "\n",
    "track_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "feature_list =  ['danceability','energy', 'loudness', 'speechiness', 'acousticness',\n",
    "                 'instrumentalness', 'liveness', 'valence','mode','key','tempo']\n",
    "\n",
    "std_scaler = joblib.load('standard_features.pkl')\n",
    "\n",
    "class Playlist():\n",
    "    def __init__(self):\n",
    "        self.name = \"Wilson's Morning Wake Up Vol. Test\"\n",
    "        self.intro_songs = []\n",
    "        self.search_results = []\n",
    "        self.recommended_track_ids = pd.DataFrame() #list of track ids straight from spotify\n",
    "        self.trax = [] #all tracks as dict\n",
    "        self.df = None #this is where the data goes\n",
    "        self.playlist = None\n",
    "        \n",
    "       \n",
    "        # DO EVERYTHING\n",
    "        self.get_recommendations() # Grab recommendations based on full WMW catalog\n",
    "        self.prep_features() # Prepare features using StandardScaler\n",
    "#         self.get_predictions() # Generate features for each track position for new WMW\n",
    "        \n",
    "        \n",
    "    def get_recommendations(self):\n",
    "        print('Getting Recommendations...')\n",
    "        \n",
    "        # Iterate full catalog of WMW songs\n",
    "        for _, row in tqdm(track_data[track_data['volume'] == 38].iterrows(), total=track_data[track_data['volume'] == 38].shape[0]):\n",
    "            song_search = row['track_name'].partition('-')[0] + ' ' + row['artist_name']\n",
    "            try:\n",
    "        \n",
    "                # Query Spotify to get track metadata\n",
    "                song_res = sp.search(song_search, limit=1)['tracks']['items'][0]\n",
    "\n",
    "                self.search_results.append({\n",
    "                    'id': song_res['id'],\n",
    "                    'artists': [i['name'] for i in song_res['artists']],\n",
    "                    'name': song_res['name']\n",
    "                })\n",
    "                \n",
    "                # Gather recommendations for each of the past WMW tracks\n",
    "                results = sp.recommendations(seed_tracks = [song_res['id']], limit=10)\n",
    "\n",
    "                for r in results['tracks']:\n",
    "                    track={}\n",
    "                    track['id'] = r['id']\n",
    "                    track['artists'] = [i['name'] for i in r['artists']],\n",
    "                    track['name'] = r['name']\n",
    "                    track_features = sp.audio_features(r['id'])[0]\n",
    "                    track.update(track_features)\n",
    "                    final_track = pd.DataFrame(track, index=[0])\n",
    "                    self.recommended_track_ids = self.recommended_track_ids.append(final_track, ignore_index=True)\n",
    "                    \n",
    "            except:\n",
    "                print(\"Song not searchable\")\n",
    "        \n",
    "        return self.recommended_track_ids\n",
    "    \n",
    "    \n",
    "    def prep_features(self):\n",
    "        self.recommended_track_ids[feature_list] = std_scaler.transform(self.recommended_track_ids[feature_list])\n",
    "            \n",
    "    \n",
    "    def generate_playlist_features(model, intro_tracks, predict_len=15):\n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # extracts features from intro tracks\n",
    "        intro_input = text_to_tensor()\n",
    "        \n",
    "        # predicted playlist\n",
    "        predicted = intro_tracks\n",
    "        \n",
    "        # build up hidden state\n",
    "        for p in range(len(intro_tracks) - 1):\n",
    "            _, hidden = model(intro_input[p], hidden)\n",
    "        inp = intro_input[-1]\n",
    "        \n",
    "        for p in range(predict_len):\n",
    "            output, hidden = model(inp, hidden)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Recommendations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ee0c63fa744ed2807917bfabe100d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:63: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "pl = Playlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.recommended_track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "feature_list =  ['danceability','energy', 'loudness', 'speechiness', 'acousticness',\n",
    "                 'instrumentalness', 'liveness', 'valence','mode','key','tempo']\n",
    "\n",
    "std_scaler = joblib.load('standard_features.pkl')\n",
    "\n",
    "def predict_playlist(model, initial_songs=[], predict_len=15):\n",
    "\n",
    "    intro_tracks = pd.DataFrame() #list of track ids straight from spotify\n",
    "    \n",
    "    # Iterate full catalog of WMW songs\n",
    "    for song in tqdm(initial_songs, total=len(initial_songs)):\n",
    "                \n",
    "        song_search = song\n",
    "                \n",
    "        try:\n",
    "            \n",
    "            # Query Spotify to get track metadata\n",
    "            song_res = sp.search(song_search, limit=1)['tracks']['items'][0]\n",
    "            \n",
    "            track = {\n",
    "                'id': song_res['id'],\n",
    "                'artists': [i['name'] for i in song_res['artists']],\n",
    "                'name': song_res['name']\n",
    "            }\n",
    "            \n",
    "            track_features = sp.audio_features(track['id'])[0]\n",
    "            \n",
    "            track.update(track_features)\n",
    "            \n",
    "            final_track = pd.DataFrame(track, index=[0])\n",
    "            \n",
    "            intro_tracks = intro_tracks.append(final_track, ignore_index=True)\n",
    "                \n",
    "        except:\n",
    "            print(\"Song not searchable\")\n",
    "\n",
    "    intro_tracks[feature_list] = std_scaler.transform(intro_tracks[feature_list])\n",
    "        \n",
    "    seq = torch.FloatTensor(intro_tracks[feature_list].values)\n",
    "    \n",
    "    predicted = seq\n",
    "\n",
    "    # build up hidden state\n",
    "    for p in range(len(initial_songs)):\n",
    "        _ = model(seq[p])\n",
    "    inp = intro_input[-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        output = model(inp)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5095ccda14f4ef1a50d3c5a549a8d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:40: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 11, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-03b38ded972b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minitial_songs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'luke howard portrait gallery'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tom misch movie'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredict_playlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_songs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_songs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-c018050a1edf>\u001b[0m in \u001b[0;36mpredict_playlist\u001b[0;34m(model, initial_songs, predict_len)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# build up hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_songs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintro_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/wmw_deep_playlist_generator/model/LSTM_Estimator.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sequence)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# define the feedforward behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mtarget_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[Tensor, Tensor], Optional[Tensor]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    157\u001b[0m             raise RuntimeError(\n\u001b[1;32m    158\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 159\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 11, got 1"
     ]
    }
   ],
   "source": [
    "initial_songs = ['luke howard portrait gallery', 'tom misch movie']\n",
    "\n",
    "predict_playlist(model, initial_songs=initial_songs, predict_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training function\n",
    "# def train_rnn(model, train_loader, epochs, criterion, optimizer, device):\n",
    "#     \"\"\"\n",
    "#     This is the training method that is called by the PyTorch training script. The parameters\n",
    "#     passed are as follows:\n",
    "#     model        - The PyTorch model that we wish to train.\n",
    "#     train_loader - The PyTorch DataLoader that should be used during training.\n",
    "#     epochs       - The total number of epochs to train for.\n",
    "#     criterion    - The loss function used for training. \n",
    "#     optimizer    - The optimizer to use during training.\n",
    "#     device       - Where the model and data should be loaded (gpu or cpu).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # training loop is provided\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         model.train() # Make sure that the model is in training mode.\n",
    "\n",
    "#         total_loss = 0\n",
    "        \n",
    "#         hidden = model.initHidden()\n",
    "\n",
    "#         for batch in train_loader:\n",
    "            \n",
    "#             # get data\n",
    "#             batch_x, batch_y = batch\n",
    "            \n",
    "#             # \n",
    "#             batch_x = torch.from_numpy(batch_x).float().squeeze()\n",
    "#             batch_y = torch.from_numpy(batch_y).float()\n",
    "\n",
    "#             batch_x = batch_x.to(device)\n",
    "#             batch_y = batch_y.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             y_pred = []\n",
    "            \n",
    "#             # get predictions\n",
    "#             for x in batch_x:\n",
    "#                 y, hidden = model(x, hidden)\n",
    "#                 y_pred.append(y)\n",
    "            \n",
    "#             # perform backprop\n",
    "#             loss = criterion(y_pred, batch_y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_loss += loss.data.item()\n",
    "            \n",
    "#         if epoch%25 == 1:\n",
    "#             print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))\n",
    "\n",
    "#TODO: Create working RNN Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# from model.RnnEstimator import RNNEstimator\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = RNNEstimator(11, 30, 8)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "# train_rnn(model, processed_data, 100, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the PyTorch Model with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator code\n",
    "from sagemaker.pytorch import PyTorch\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "estimator = PyTorch(entry_point=\"LSTM_Train.py\",\n",
    "                    source_dir=\"model\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    output_path = output_path,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'input_features': 11,\n",
    "                        'hidden_dim': 12,\n",
    "                        'output_dim': 8,\n",
    "                        'epochs': 100\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-05 03:44:03 Starting - Starting the training job...\n",
      "2020-03-05 03:44:04 Starting - Launching requested ML instances.........\n",
      "2020-03-05 03:45:34 Starting - Preparing the instances for training.........\n",
      "2020-03-05 03:47:17 Downloading - Downloading input data\n",
      "2020-03-05 03:47:17 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:37,157 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:37,160 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:37,172 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:37,176 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:37,390 sagemaker-containers INFO     Module LSTM_Train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:37,390 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:37,391 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:37,391 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: LSTM-Train\n",
      "  Running setup.py bdist_wheel for LSTM-Train: started\n",
      "  Running setup.py bdist_wheel for LSTM-Train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-awuo44mk/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built LSTM-Train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: LSTM-Train\u001b[0m\n",
      "\u001b[34mSuccessfully installed LSTM-Train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:38,793 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:38,807 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"user_entry_point\": \"LSTM_Train.py\",\n",
      "    \"log_level\": 20,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-03-05-03-44-02-776\",\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 12,\n",
      "        \"epochs\": 100,\n",
      "        \"output_dim\": 8,\n",
      "        \"input_features\": 11\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-999752527953/sagemaker-pytorch-2020-03-05-03-44-02-776/source/sourcedir.tar.gz\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"module_name\": \"LSTM_Train\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=12\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":100,\"hidden_dim\":12,\"input_features\":11,\"output_dim\":8}\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_FEATURES=11\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":100,\"hidden_dim\":12,\"input_features\":11,\"output_dim\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-03-05-03-44-02-776\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-999752527953/sagemaker-pytorch-2020-03-05-03-44-02-776/source/sourcedir.tar.gz\",\"module_name\":\"LSTM_Train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"LSTM_Train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"100\",\"--hidden_dim\",\"12\",\"--input_features\",\"11\",\"--output_dim\",\"8\"]\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=8\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-999752527953/sagemaker-pytorch-2020-03-05-03-44-02-776/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=LSTM_Train\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=LSTM_Train.py\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m LSTM_Train --epochs 100 --hidden_dim 12 --input_features 11 --output_dim 8\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.15.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, pandas\u001b[0m\n",
      "\u001b[34mSuccessfully installed pandas-0.24.2 pytz-2019.3\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data.\u001b[0m\n",
      "\u001b[34mEpoch: 1, Loss: 0.8083903950613898\u001b[0m\n",
      "\u001b[34mEpoch: 26, Loss: 0.2432150756185119\u001b[0m\n",
      "\n",
      "2020-03-05 03:48:02 Uploading - Uploading generated training model\u001b[34mEpoch: 51, Loss: 0.13623916377892364\u001b[0m\n",
      "\u001b[34mEpoch: 76, Loss: 0.0919963034826356\u001b[0m\n",
      "\u001b[34m2020-03-05 03:47:56,812 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-05 03:48:09 Completed - Training job completed\n",
      "Training seconds: 54\n",
      "Billable seconds: 54\n"
     ]
    }
   ],
   "source": [
    "# Fit estimator\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!CPU times: user 254 ms, sys: 15.1 ms, total: 270 ms\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3910e+00, -2.6351e+00, -2.7732e-01,  9.2970e-01, -4.8984e-01,\n",
       "         -1.1569e+00,  1.0857e+00, -1.2045e+00,  2.0962e+00, -5.3704e+00,\n",
       "          2.3380e-01],\n",
       "        [ 2.9102e-01, -1.3109e-01, -3.5296e-01,  6.4590e-01, -2.8399e-01,\n",
       "         -1.2541e-01, -9.2107e-01,  1.4323e+00,  6.8586e-01, -3.7562e-01,\n",
       "          6.8282e-01],\n",
       "        [ 3.8522e-01,  9.4912e-01,  6.0842e-02, -1.4610e+00,  1.5104e+00,\n",
       "          2.6102e+00, -9.2107e-01,  1.7253e+00, -7.7038e-01,  1.8358e-01,\n",
       "         -1.1925e+00],\n",
       "        [-4.4044e-01,  6.0751e-01, -5.4846e-02, -1.4123e+00, -1.6747e-01,\n",
       "          1.5519e+00,  1.0857e+00, -1.4975e+00, -8.6166e-01,  8.8285e-01,\n",
       "          3.2336e+00],\n",
       "        [ 4.9051e-01, -1.8348e-03, -6.1326e-01, -9.6511e-01, -5.3411e-01,\n",
       "         -5.2279e-01, -9.2107e-01,  5.5335e-01, -8.1149e-01,  5.3856e-01,\n",
       "         -9.0695e-01],\n",
       "        [ 6.9554e-01,  1.2076e+00, -5.0396e-02,  8.1217e-01, -7.6404e-01,\n",
       "         -7.5214e-02,  1.0857e+00, -3.2617e-02,  3.1377e-01,  1.0708e-01,\n",
       "         -4.1025e-02],\n",
       "        [ 5.7363e-01,  1.0414e+00,  3.9233e-01,  7.3477e-01, -2.6845e-01,\n",
       "          2.4687e-01,  1.0857e+00,  1.7253e+00, -5.2316e-02,  6.3584e-01,\n",
       "          2.4565e+00],\n",
       "        [ 1.2995e+00, -8.0045e-01,  6.6820e-01,  2.5032e-01, -5.9315e-01,\n",
       "         -1.0808e+00, -9.2107e-01,  2.6037e-01,  1.2050e+00,  5.6468e-01,\n",
       "          7.7759e-01],\n",
       "        [-3.9611e-01,  3.0479e-02, -1.6608e-01, -1.3865e+00,  4.1512e-01,\n",
       "         -1.7142e-01, -9.2107e-01,  2.6037e-01,  2.2675e-01,  6.0109e-01,\n",
       "         -2.0609e+00],\n",
       "        [ 3.8522e-01,  3.5823e-01, -4.7978e-01, -5.5233e-01, -4.0827e-01,\n",
       "         -1.5051e-01, -9.2107e-01,  1.1393e+00,  2.3275e-01,  5.0928e-02,\n",
       "          1.5102e-01],\n",
       "        [ 9.4490e-01, -6.3888e-01,  2.0876e+00, -1.6795e+00, -3.6943e-01,\n",
       "         -6.1899e-01, -9.2107e-01, -9.1157e-01,  1.1870e+00,  3.1171e-01,\n",
       "          1.9953e-01],\n",
       "        [-4.1828e-01,  4.3671e-01, -4.3306e-01,  1.8152e-01,  2.2406e+00,\n",
       "         -8.5324e-01, -9.2107e-01,  1.1393e+00, -8.7952e-01, -3.2984e-02,\n",
       "          4.3956e-01],\n",
       "        [ 7.4541e-01, -1.1836e+00, -2.0613e-01,  6.2297e-01, -6.9646e-01,\n",
       "         -7.2357e-01,  1.0857e+00, -9.1157e-01, -8.6283e-01, -1.1055e-03,\n",
       "          5.6152e-02],\n",
       "        [ 2.7439e-01,  6.4906e-01, -2.6397e-01,  3.3058e-01,  3.2968e-01,\n",
       "          1.1629e+00,  1.0857e+00,  1.7253e+00, -5.9724e-01,  5.9019e-01,\n",
       "         -1.8527e-01],\n",
       "        [ 1.8573e-01,  8.1986e-01, -4.0636e-01, -1.0227e-01, -4.0827e-01,\n",
       "         -1.1465e+00, -9.2107e-01,  2.6037e-01, -5.5043e-01,  5.5419e-01,\n",
       "          1.5280e-01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(processed_data[0][0]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.predict(torch.Tensor(processed_data[0][0]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3910, -2.6351, -0.2773,  0.9297, -0.4898, -1.1569,  1.0857, -1.2045,\n",
       "         2.0962, -5.3704,  0.2338])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(new_tracks[-1]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.3909948690196825,\n",
       " -2.635094590468726,\n",
       " -0.2773220412902482,\n",
       " 0.9296953263811546,\n",
       " -0.4898368594156362,\n",
       " -1.1569194705342014,\n",
       " 1.0856902892884872,\n",
       " -1.2045490262286025,\n",
       " 2.0961845785776654,\n",
       " -5.370441776536932,\n",
       " 0.23380331292868914]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fut_pred = processed_data[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.3910, -2.6351, -0.2773,  0.9297, -0.4898, -1.1569,  1.0857, -1.2045,\n",
      "         2.0962, -5.3704,  0.2338])\n"
     ]
    },
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request.  Either the server is overloaded or there is an error in the application.</p>\n\". See https://ap-southeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-2#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-pytorch-2020-03-05-03-44-02-776 in account 999752527953 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-1a12d06da287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tracks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# for i in range(playlist_len - len(fut_pred)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#         print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request.  Either the server is overloaded or there is an error in the application.</p>\n\". See https://ap-southeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-southeast-2#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-pytorch-2020-03-05-03-44-02-776 in account 999752527953 for more information."
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "\n",
    "fut_pred = processed_data[0][0][0]\n",
    "\n",
    "playlist_len = 15\n",
    "\n",
    "new_tracks = [torch.Tensor(fut_pred).float()]\n",
    "\n",
    "print(new_tracks[-1])\n",
    "\n",
    "\n",
    "predictor.predict(new_tracks[-1])\n",
    "\n",
    "# for i in range(playlist_len - len(fut_pred)):\n",
    "#         print(i)\n",
    "#         print(predictor.predict(new_tracks[-1].values))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
